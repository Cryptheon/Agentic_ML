# config.yaml
llm:
  provider: "ollama"
  model_id: "ollama_chat/qwen3:1.7b" #Qwen/Qwen3-14B"
  transformers_model_kwargs:
    kwargs: # These are passed to AutoModelForCausalLM.from_pretrained
      device_map: "auto" # Example
      # torch_dtype: "auto" # Example
      max_new_tokens: 16384

agent:
  max_steps: 40
  stream_outputs: true
  additional_authorized_imports:
    - "torch"
    - "numpy"
    - "pandas"
    - "os"
    - "json"
    - "logging"

paths:
  initial_prompt_json: "../configs/initial_project_config.json" # Path to JSON file with initial prompt details

