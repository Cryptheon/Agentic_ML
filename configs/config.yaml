llm:
  provider: "openai" #"ollama"
  model_id: "gemini-2.0-flash" #"ollama_chat/qwen3:32b" #Qwen/Qwen3-14B" #ollama_chat/qwen3:14b # gemini-2.0-flash
  transformers_model_kwargs:
    kwargs: # These are passed to AutoModelForCausalLM.from_pretrained
      device_map: "auto" # Example
      # torch_dtype: "auto" # Example
      max_new_tokens: 16384

agent:
  max_steps: 16
  stream_outputs: true
  additional_authorized_imports:
    - "torch"
    - "numpy"
    - "pandas"
    - "os"
    - "json"
    - "logging"
    - "posixpath"
    - "open"

run:
  initial_prompt_json: "../configs/initial_project_config.json" # Path to JSON file with initial prompt details
  execute_phase: "all"

